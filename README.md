# ğŸ‡»ğŸ‡³ Vietnamese Text Summarization with SFT & DPO  
**Fine-tuning Qwen / Qwen3-0.6B**

## ğŸ“Œ Giá»›i thiá»‡u
Repository nÃ y thá»±c hiá»‡n **fine-tuning mÃ´ hÃ¬nh ngÃ´n ngá»¯ Qwen/Qwen3-0.6B cho bÃ i toÃ¡n tÃ³m táº¯t vÄƒn báº£n tiáº¿ng Viá»‡t**, bao gá»“m hai giai Ä‘oáº¡n chÃ­nh:
- **Supervised Fine-Tuning (SFT)**
- **Direct Preference Optimization (DPO)**

Má»¥c tiÃªu lÃ  so sÃ¡nh vÃ  Ä‘Ã¡nh giÃ¡ hiá»‡u quáº£ cá»§a **DPO so vá»›i SFT** trong viá»‡c cáº£i thiá»‡n cháº¥t lÆ°á»£ng báº£n tÃ³m táº¯t.

---

## ğŸ§  MÃ´ hÃ¬nh
- **Base model**: `Qwen/Qwen3-0.6B`
- Quy mÃ´ nhá» (~0.6B tham sá»‘), phÃ¹ há»£p cho thá»±c nghiá»‡m vÃ  fine-tuning trÃªn tÃ i nguyÃªn háº¡n cháº¿.

---

## ğŸ“Š Dá»¯ liá»‡u
- **Dataset**: `OpenHust/vietnamese-summarization`
- **Táº­p dá»¯ liá»‡u dÃ¹ng cho DPO**:
```python
from datasets import load_dataset
ds = load_dataset("OpenHust/vietnamese-summarization", split="train[:300]")

## Cáº¥u trÃºc thÆ° má»¥c
â”œâ”€â”€ SFT.ipynb                  # Supervised Fine-Tuning
â”œâ”€â”€ DPO_new.ipynb              # Direct Preference Optimization
â”œâ”€â”€ testmodel.ipynb            # Thá»­ nghiá»‡m sinh tÃ³m táº¯t
â”œâ”€â”€ dpo_dataset.csv            # Dá»¯ liá»‡u DPO (chosen / rejected)
â”œâ”€â”€ summarization_results.csv  # Káº¿t quáº£ tÃ³m táº¯t
â”œâ”€â”€ test.csv                   # Dá»¯ liá»‡u kiá»ƒm thá»­
â”œâ”€â”€ RUNS.txt                   # Ghi chÃº cÃ¡c láº§n cháº¡y
â”œâ”€â”€ Report.docx                # BÃ¡o cÃ¡o
â””â”€â”€ Report.pdf                 # BÃ¡o cÃ¡o (PDF)
