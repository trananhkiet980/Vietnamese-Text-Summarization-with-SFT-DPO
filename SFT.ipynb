{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821b1c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 1. LOAD MODEL & TOKENIZER (LoRA)\n",
    "# ===============================\n",
    "def load_model(model_name=\"Qwen/Qwen3-0.6B\", device=\"cuda\"):\n",
    "    print(f\"[INFO] Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    \n",
    "    # Th√™m pad token n·∫øu ch∆∞a c√≥\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        load_in_4bit=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    print(\"[INFO] LoRA layers attached successfully.\")\n",
    "    model.print_trainable_parameters()\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 2. LOAD & PREPROCESS DATA - FIXED PADDING\n",
    "# ===============================\n",
    "def load_and_prepare_data(tokenizer, split_ratio=\"train[:3%]\"):\n",
    "    print(\"[INFO] Loading dataset...\")\n",
    "    ds = load_dataset(\"OpenHust/vietnamese-summarization\", split=split_ratio)\n",
    "    \n",
    "    text_column = 'Document'\n",
    "    summary_column = 'Summary'\n",
    "    \n",
    "    print(f\"[INFO] Dataset size: {len(ds)} samples\")\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        texts = []\n",
    "        \n",
    "        for i in range(len(examples[text_column])):\n",
    "            input_text = examples[text_column][i]\n",
    "            summary_text = examples[summary_column][i]\n",
    "            \n",
    "            # T·∫°o m·ªôt chu·ªói duy nh·∫•t: input + summary\n",
    "            full_text = f\"### Input:\\n{input_text}\\n\\n### Summary:\\n{summary_text}{tokenizer.eos_token}\"\n",
    "            texts.append(full_text)\n",
    "        \n",
    "        # Tokenize v·ªõi padding ƒë·ªÉ t·∫•t c·∫£ sequences c√≥ c√πng ƒë·ªô d√†i\n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            max_length=1024,  # Gi·ªõi h·∫°n ƒë·ªô d√†i\n",
    "            truncation=True,\n",
    "            padding=True,     # QUAN TR·ªåNG: Th√™m padding\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        # Labels ch√≠nh l√† input_ids\n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        \n",
    "        return tokenized\n",
    "\n",
    "    tokenized_ds = ds.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=ds.column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    # Ph√¢n t√≠ch k·∫øt qu·∫£ tokenization\n",
    "    input_lengths = [len(x['input_ids']) for x in tokenized_ds]\n",
    "    \n",
    "    print(f\"\\nüìä TOKENIZATION STATS:\")\n",
    "    print(f\"Total tokens - Avg: {sum(input_lengths)/len(input_lengths):.1f}, Max: {max(input_lengths)}, Min: {min(input_lengths)}\")\n",
    "    \n",
    "    # Ki·ªÉm tra xem t·∫•t c·∫£ sequences c√≥ c√πng ƒë·ªô d√†i kh√¥ng\n",
    "    unique_lengths = set(input_lengths)\n",
    "    print(f\"Unique sequence lengths: {unique_lengths}\")\n",
    "    \n",
    "    return tokenized_ds\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 3. TRAINING SETUP - FIXED\n",
    "# ===============================\n",
    "def setup_training_args(OUTPUT_DIR=\"./runs/qwen3-0.6B-summarization\", NUM_EPOCHS=3):\n",
    "    os.makedirs(\"runs\", exist_ok=True)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=NUM_EPOCHS,\n",
    "        learning_rate=1e-4,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=True,  # ƒê·∫∑t True ƒë·ªÉ trainer t·ª± x·ª≠ l√Ω\n",
    "        dataloader_pin_memory=False,\n",
    "        gradient_checkpointing=True,\n",
    "        max_grad_norm=0.3,\n",
    "        dataloader_num_workers=0,\n",
    "        # Th√™m c·∫•u h√¨nh ƒë·ªÉ tr√°nh l·ªói padding\n",
    "        group_by_length=False,  # QUAN TR·ªåNG: Kh√¥ng group by length\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 4. DATA COLLATOR - SIMPLIFIED\n",
    "# ===============================\n",
    "def create_data_collator(tokenizer):\n",
    "    return DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer,\n",
    "        mlm=False,\n",
    "        pad_to_multiple_of=8,\n",
    "    )\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 5. TRAIN FUNCTION - IMPROVED DEBUG\n",
    "# ===============================\n",
    "def train_model(model, tokenizer, dataset, training_args, SAVE_DIR=\"./runs/qwen3-0.6B-summarization-lora\"):\n",
    "    print(\"[INFO] Initializing Trainer...\")\n",
    "\n",
    "    data_collator = create_data_collator(tokenizer)\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    print(\"[INFO] Start training with LoRA...\")\n",
    "    \n",
    "    # Ki·ªÉm tra dataset tr∆∞·ªõc\n",
    "    print(\"[DEBUG] Dataset info:\")\n",
    "    print(f\"  - Number of samples: {len(dataset)}\")\n",
    "    print(f\"  - Features: {dataset.features}\")\n",
    "    \n",
    "    # Ki·ªÉm tra m·ªôt sample\n",
    "    print(\"[DEBUG] First sample structure:\")\n",
    "    first_sample = dataset[0]\n",
    "    for key in first_sample.keys():\n",
    "        print(f\"  - {key}: {type(first_sample[key])}, length: {len(first_sample[key])}\")\n",
    "    \n",
    "    # Ki·ªÉm tra data collator\n",
    "    print(\"[DEBUG] Testing data collator with 2 samples...\")\n",
    "    test_samples = [dataset[0], dataset[1]]\n",
    "    try:\n",
    "        collated_batch = data_collator(test_samples)\n",
    "        print(f\"[DEBUG] Collated batch shapes:\")\n",
    "        for key, value in collated_batch.items():\n",
    "            print(f\"  - {key}: {value.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Data collator failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Ki·ªÉm tra dataloader\n",
    "    print(\"[DEBUG] Checking dataloader...\")\n",
    "    try:\n",
    "        train_dataloader = trainer.get_train_dataloader()\n",
    "        sample_batch = next(iter(train_dataloader))\n",
    "        \n",
    "        print(f\"[DEBUG] Batch shapes:\")\n",
    "        print(f\"  - Input IDs: {sample_batch['input_ids'].shape}\")\n",
    "        print(f\"  - Labels: {sample_batch['labels'].shape}\")\n",
    "        print(f\"  - Attention mask: {sample_batch['attention_mask'].shape}\")\n",
    "        \n",
    "        # Ki·ªÉm tra n·ªôi dung\n",
    "        print(f\"[DEBUG] Sample input (first 30 tokens):\")\n",
    "        print(tokenizer.decode(sample_batch['input_ids'][0][:30]))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Dataloader failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # B·∫Øt ƒë·∫ßu training\n",
    "    try:\n",
    "        result = trainer.train()\n",
    "        \n",
    "        # L∆∞u model\n",
    "        save_dir = SAVE_DIR\n",
    "        model.save_pretrained(save_dir)\n",
    "        tokenizer.save_pretrained(save_dir)\n",
    "        print(f\"[INFO] LoRA adapter saved to {save_dir}\")\n",
    "        \n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Training failed: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 6. TEST INFERENCE\n",
    "# ===============================\n",
    "def test_inference(model, tokenizer, test_text):\n",
    "    \"\"\"Test model sau khi training\"\"\"\n",
    "    prompt = f\"### Input:\\n{test_text}\\n\\n### Summary:\\n\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=1024, truncation=True).to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=150,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    full_response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Tr√≠ch xu·∫•t ph·∫ßn t√≥m t·∫Øt\n",
    "    if \"### Summary:\" in full_response:\n",
    "        summary = full_response.split(\"### Summary:\")[-1].strip()\n",
    "    else:\n",
    "        summary = full_response\n",
    "    \n",
    "    print(f\"[TEST] Input length: {len(test_text)} chars\")\n",
    "    print(f\"[TEST] Input preview: {test_text[:100]}...\")\n",
    "    print(f\"[TEST] Generated summary: {summary}\")\n",
    "    return summary\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 7. ALTERNATIVE APPROACH - Dynamic Padding\n",
    "# ===============================\n",
    "def load_and_prepare_data_dynamic(tokenizer, split_ratio=\"train[:3%]\"):\n",
    "    \"\"\"Alternative approach v·ªõi dynamic padding\"\"\"\n",
    "    print(\"[INFO] Loading dataset with dynamic padding approach...\")\n",
    "    ds = load_dataset(\"OpenHust/vietnamese-summarization\", split=split_ratio)\n",
    "    \n",
    "    text_column = 'Document'\n",
    "    summary_column = 'Summary'\n",
    "\n",
    "    def preprocess_function(examples):\n",
    "        texts = []\n",
    "        \n",
    "        for i in range(len(examples[text_column])):\n",
    "            input_text = examples[text_column][i]\n",
    "            summary_text = examples[summary_column][i]\n",
    "            \n",
    "            full_text = f\"### Input:\\n{input_text}\\n\\n### Summary:\\n{summary_text}{tokenizer.eos_token}\"\n",
    "            texts.append(full_text)\n",
    "        \n",
    "        # Tokenize KH√îNG padding ·ªü ƒë√¢y\n",
    "        tokenized = tokenizer(\n",
    "            texts,\n",
    "            max_length=1024,\n",
    "            truncation=True,\n",
    "            padding=False,  # Kh√¥ng padding ·ªü preprocessing\n",
    "            return_tensors=None\n",
    "        )\n",
    "        \n",
    "        tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "        return tokenized\n",
    "\n",
    "    tokenized_ds = ds.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        batch_size=1000,\n",
    "        remove_columns=ds.column_names,\n",
    "        num_proc=1\n",
    "    )\n",
    "\n",
    "    return tokenized_ds\n",
    "\n",
    "\n",
    "# ===============================\n",
    "# 8. MAIN - FLEXIBLE\n",
    "# ===============================\n",
    "# if __name__ == \"__main__\":\n",
    "#     print(\"üöÄ Starting training...\")\n",
    "#     model, tokenizer = load_model()\n",
    "    \n",
    "#     # Th·ª≠ c·∫£ hai c√°ch ti·∫øp c·∫≠n\n",
    "#     print(\"üîß Testing data preprocessing...\")\n",
    "    \n",
    "#     # C√°ch 1: Padding trong preprocessing\n",
    "#     try:\n",
    "#         tokenized_ds = load_and_prepare_data(tokenizer, \"train[:1%]\")\n",
    "#         print(\"‚úÖ Approach 1 (pre-padding) successful\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Approach 1 failed: {e}\")\n",
    "#         # Th·ª≠ c√°ch 2\n",
    "#         print(\"üîÑ Trying approach 2 (dynamic padding)...\")\n",
    "#         tokenized_ds = load_and_prepare_data_dynamic(tokenizer, \"train[:1%]\")\n",
    "#         print(\"‚úÖ Approach 2 (dynamic padding) successful\")\n",
    "    \n",
    "#     # Test inference tr∆∞·ªõc training\n",
    "#     print(\"\\nüß™ Testing inference before training...\")\n",
    "#     test_sample = \"Vi·ªát Nam l√† m·ªôt qu·ªëc gia n·∫±m ·ªü khu v·ª±c ƒê√¥ng Nam √Å, c√≥ b·ªù bi·ªÉn d√†i v√† n·ªÅn vƒÉn h√≥a phong ph√∫.\"\n",
    "#     test_inference(model, tokenizer, test_sample)\n",
    "    \n",
    "#     training_args = setup_training_args(NUM_EPOCHS=2)\n",
    "#     result = train_model(model, tokenizer, tokenized_ds, training_args)\n",
    "    \n",
    "#     if result:\n",
    "#         # Test inference sau training\n",
    "#         print(\"\\nüß™ Testing inference after training...\")\n",
    "#         test_inference(model, tokenizer, test_sample)\n",
    "#     else:\n",
    "#         print(\"‚ùå Training failed, skipping inference test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c29242b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading model: Qwen/Qwen3-0.6B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] LoRA layers attached successfully.\n",
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7638\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1840823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] Testing inference before training...\n",
      "[TEST] Input length: 177 chars\n",
      "[TEST] Input preview: Vi·ªát Nam l√† m·ªôt qu·ªëc gia n·∫±m ·ªü khu v·ª±c ƒê√¥ng Nam √Å, c√≥ b·ªù bi·ªÉn d√†i v√† n·ªÅn vƒÉn h√≥a phong ph√∫. ƒê·∫•t n∆∞·ªõc...\n",
      "[TEST] Generated summary: T√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt.\n",
      "\n",
      "---\n",
      "\n",
      "**Output Format:**\n",
      "- Start with `Summary:` followed by the summary in Vietnamese.\n",
      "- If there are multiple sentences or paragraphs, ensure that they follow a continuous format.\n",
      "\n",
      "---\n",
      "\n",
      "**Example:**\n",
      "- Summary: \"Vietnam is located in Southeast Asia, with long and wide coastlines and rich cultural traditions.\"\n",
      "\n",
      "---\n",
      "Okay, let's see. The user provided a passage about Vietnam's geographical location and history, and wants me to summarize it into one sentence. Let me check again.\n",
      "\n",
      "The original input is:\n",
      "\n",
      "\"Vi·ªát Nam l√† m·ªôt qu·ªëc gia n·∫±m ·ªü khu v·ª±c ƒê√¥ng Nam √Å, c√≥ b·ªù bi·ªÉn d√†i v√† n·ªÅn vƒÉn h√≥a phong ph√∫. ƒê·∫•t n∆∞·ªõc n√†y c√≥ l·ªãch s·ª≠\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'T√≥m t·∫Øt n·ªôi dung ch√≠nh c·ªßa b√†i vi·∫øt.\\n\\n---\\n\\n**Output Format:**\\n- Start with `Summary:` followed by the summary in Vietnamese.\\n- If there are multiple sentences or paragraphs, ensure that they follow a continuous format.\\n\\n---\\n\\n**Example:**\\n- Summary: \"Vietnam is located in Southeast Asia, with long and wide coastlines and rich cultural traditions.\"\\n\\n---\\nOkay, let\\'s see. The user provided a passage about Vietnam\\'s geographical location and history, and wants me to summarize it into one sentence. Let me check again.\\n\\nThe original input is:\\n\\n\"Vi·ªát Nam l√† m·ªôt qu·ªëc gia n·∫±m ·ªü khu v·ª±c ƒê√¥ng Nam √Å, c√≥ b·ªù bi·ªÉn d√†i v√† n·ªÅn vƒÉn h√≥a phong ph√∫. ƒê·∫•t n∆∞·ªõc n√†y c√≥ l·ªãch s·ª≠'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test inference tr∆∞·ªõc khi train\n",
    "print(\"\\n[INFO] Testing inference before training...\")\n",
    "test_sample = \"Vi·ªát Nam l√† m·ªôt qu·ªëc gia n·∫±m ·ªü khu v·ª±c ƒê√¥ng Nam √Å, c√≥ b·ªù bi·ªÉn d√†i v√† n·ªÅn vƒÉn h√≥a phong ph√∫. ƒê·∫•t n∆∞·ªõc n√†y c√≥ l·ªãch s·ª≠ l√¢u ƒë·ªùi v√† ƒë√£ tr·∫£i qua nhi·ªÅu giai ƒëo·∫°n ph√°t tri·ªÉn quan tr·ªçng.\"\n",
    "test_inference(model, tokenizer, test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4363215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading dataset...\n",
      "[INFO] Dataset size: 2237 samples\n",
      "\n",
      "üìä TOKENIZATION STATS:\n",
      "Total tokens - Avg: 1024.0, Max: 1024, Min: 1024\n",
      "Unique sequence lengths: {1024}\n"
     ]
    }
   ],
   "source": [
    "tokenized_ds = load_and_prepare_data(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df278207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2237\n",
      "})\n",
      "2237\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_ds)\n",
    "print(len(tokenized_ds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b3a1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = setup_training_args(OUTPUT_DIR=\"./runs/qwen3-0.6B-summary\", NUM_EPOCHS=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f9da994",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_7960\\3896901753.py:156: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Initializing Trainer...\n",
      "[INFO] Start training with LoRA...\n",
      "[DEBUG] Dataset info:\n",
      "  - Number of samples: 2237\n",
      "  - Features: {'input_ids': List(Value('int32')), 'attention_mask': List(Value('int8')), 'labels': List(Value('int64'))}\n",
      "[DEBUG] First sample structure:\n",
      "  - input_ids: <class 'list'>, length: 1024\n",
      "  - attention_mask: <class 'list'>, length: 1024\n",
      "  - labels: <class 'list'>, length: 1024\n",
      "[DEBUG] Testing data collator with 2 samples...\n",
      "[DEBUG] Collated batch shapes:\n",
      "  - input_ids: torch.Size([2, 1024])\n",
      "  - attention_mask: torch.Size([2, 1024])\n",
      "  - labels: torch.Size([2, 1024])\n",
      "[DEBUG] Checking dataloader...\n",
      "[DEBUG] Batch shapes:\n",
      "  - Input IDs: torch.Size([2, 1024])\n",
      "  - Labels: torch.Size([2, 1024])\n",
      "  - Attention mask: torch.Size([2, 1024])\n",
      "[DEBUG] Sample input (first 30 tokens):\n",
      "### Input:\n",
      "Theo trang Technology Review , th√†nh t·ª±u nghi√™n c·ª©u m·ªõi c·ªßa c√°c nh√† khoa h·ªçc thu·ªôc ƒê·∫°i h·ªçc Yale ( M·ªπ ) ƒë∆∞·ª£c\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='560' max='560' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [560/560 57:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.524200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.417000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.022100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.918200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.935400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.876100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.836400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.808000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.759200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.733100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.733800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.760400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.721800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>2.701100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>2.701300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>2.732700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.662400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.648800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>2.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>2.690900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>2.546200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.651300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>2.649300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>2.640700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>2.674700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>2.609100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.605300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>2.614300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>2.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>2.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>2.740300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>2.618500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>2.611500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>2.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>2.620200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.610700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>2.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>2.538100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>2.600700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>2.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.586400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>2.576000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>2.602700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>2.663300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>2.647600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.551900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>2.569300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] LoRA adapter saved to ./runs/qwen3-0.6B-summarization-lora\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=560, training_loss=2.7229151453290665, metrics={'train_runtime': 3437.6932, 'train_samples_per_second': 1.301, 'train_steps_per_second': 0.163, 'total_flos': 1.2233785114361856e+16, 'train_loss': 2.7229151453290665, 'epoch': 2.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(model, tokenizer, tokenized_ds, training_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4202927b",
   "metadata": {},
   "source": [
    "## TEST BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d06dce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt qu·∫£ model nguy√™n b·∫£n: - C√°c m√≥n ƒÉn ·ªü mi·ªÅn B·∫Øc c√≥ ƒë·∫∑c ƒëi·ªÉm g√¨?\n",
      "- C√°c m√≥n ƒÉn ·ªü mi·ªÅn Nam c√≥ ƒë·∫∑c ƒëi·ªÉm g√¨?\n",
      "- C√°c m√≥n ƒÉn ·ªü mi·ªÅn Trung c√≥ ƒë·∫∑c ƒëi·ªÉm g√¨?\n",
      "- Nh·ªØng m√≥n ƒÉn n·ªïi ti·∫øng nh·∫•t ·ªü m·ªói mi·ªÅn n√†o?\n",
      "\n",
      "**T√≥m t·∫Øt ng·∫Øn g·ªçn**:\n",
      "- Mi·ªÅn B·∫Øc n·ªïi ti·∫øng v·ªõi ph·ªü, b√∫n ch·∫£, b√∫n b√≤ Hu·∫ø, cao l·∫ßu.\n",
      "- Mi·ªÅn Nam n·ªïi ti·∫øng v·ªõi h·ªß ti·∫øu, c∆°m t·∫•m\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model nguy√™n b·∫£n\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-0.6B\")\n",
    "\n",
    "text1 = \"\"\"·∫®m th·ª±c Vi·ªát Nam r·∫•t ƒëa d·∫°ng v√† phong ph√∫, v·ªõi nhi·ªÅu m√≥n ƒÉn ƒë·∫∑c tr∆∞ng theo t·ª´ng v√πng mi·ªÅn. \n",
    "Mi·ªÅn B·∫Øc n·ªïi ti·∫øng v·ªõi ph·ªü, b√∫n ch·∫£, mi·ªÅn Trung c√≥ b√∫n b√≤ Hu·∫ø, cao l·∫ßu, c√≤n mi·ªÅn Nam c√≥ h·ªß ti·∫øu, c∆°m t·∫•m. \n",
    "C√°c m√≥n ƒÉn Vi·ªát Nam th∆∞·ªùng ch√∫ tr·ªçng s·ª± c√¢n b·∫±ng gi·ªØa c√°c v·ªã chua, cay, m·∫∑n, ng·ªçt v√† s·ª≠ d·ª•ng nhi·ªÅu rau th∆°m, gia v·ªã t∆∞∆°i.\"\"\"\n",
    "\n",
    "prompt = f\"VƒÉn b·∫£n: {text1}\\nT√≥m t·∫Øt:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"K·∫øt qu·∫£ model nguy√™n b·∫£n:\", result.split(\"T√≥m t·∫Øt:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f6dd6a",
   "metadata": {},
   "source": [
    "## TEST SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c6d486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code test nhanh\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\", device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, \"./runs/qwen3-0.6B-summarization-lora\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./runs/qwen3-0.6B-summarization-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7839ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt qu·∫£: C√°c m√≥n ƒÉn Vi·ªát Nam c√≥ nhi·ªÅu ƒë·∫∑c tr∆∞ng theo t·ª´ng v√πng mi·ªÅn, ƒë·∫∑c tr∆∞ng cho s·ª± c√¢n b·∫±ng gi·ªØa c√°c v·ªã chua, cay, m·∫∑n, ng·ªçt v√† s·ª≠ d·ª•ng nhi·ªÅu rau th∆°m, gia v·ªã t∆∞∆°i. \n",
      "\n",
      "Tr√¨nh b√†y : ·ªû Vi·ªát Nam, m√≥n ƒÉn th∆∞·ªùng ƒë∆∞·ª£c ch·∫ø bi·∫øn b·∫±ng nhi·ªÅu ph∆∞∆°ng ph√°p kh√°c nhau. Trong ƒë√≥, c√≥ ng∆∞·ªùi d√πng nhi·ªÅu m√≥n ƒÉn c√≥ chung m·ªôt m√≥n ƒÉn. M·ªôt s·ªë m√≥n ƒÉn ƒë∆∞·ª£c ch·∫ø bi·∫øn b·ªüi ng∆∞·ªùi d√¢n Vi·ªát\n"
     ]
    }
   ],
   "source": [
    "text1 = \"\"\"·∫®m th·ª±c Vi·ªát Nam r·∫•t ƒëa d·∫°ng v√† phong ph√∫, v·ªõi nhi·ªÅu m√≥n ƒÉn ƒë·∫∑c tr∆∞ng theo t·ª´ng v√πng mi·ªÅn. \n",
    "Mi·ªÅn B·∫Øc n·ªïi ti·∫øng v·ªõi ph·ªü, b√∫n ch·∫£, mi·ªÅn Trung c√≥ b√∫n b√≤ Hu·∫ø, cao l·∫ßu, c√≤n mi·ªÅn Nam c√≥ h·ªß ti·∫øu, c∆°m t·∫•m. \n",
    "C√°c m√≥n ƒÉn Vi·ªát Nam th∆∞·ªùng ch√∫ tr·ªçng s·ª± c√¢n b·∫±ng gi·ªØa c√°c v·ªã chua, cay, m·∫∑n, ng·ªçt v√† s·ª≠ d·ª•ng nhi·ªÅu rau th∆°m, gia v·ªã t∆∞∆°i.\"\"\"\n",
    "prompt = f\"VƒÉn b·∫£n: {text1}\\nT√≥m t·∫Øt:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"K·∫øt qu·∫£:\", result.split(\"T√≥m t·∫Øt:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7480716c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt qu·∫£: Vi·ªát Nam c√≥ t·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng GDP trung b√¨nh 6-7% m·ªói nƒÉm v·ªõi c√°c ng√†nh c√¥ng nghi·ªáp ch·ªß l·ª±c. C√°c hi·ªáp ƒë·ªãnh th∆∞∆°ng m·∫°i t·ª± do m·ªü r·ªông th·ªã tr∆∞·ªùng xu·∫•t kh·∫©u. \n",
      "\n",
      "K·∫øt lu·∫≠n: TƒÉng tr∆∞·ªüng GDP c·ªßa Vi·ªát Nam trong nƒÉm 2020 ƒë∆∞·ª£c ƒë√°nh gi√° l√† t·ªët h∆°n nƒÉm 2019 v√† 2018. TƒÉng tr∆∞·ªüng ƒë·∫°t 5,3% trong nƒÉm 2019, 5,4%\n"
     ]
    }
   ],
   "source": [
    "text2 = \"\"\"N·ªÅn kinh t·∫ø Vi·ªát Nam trong nh·ªØng nƒÉm g·∫ßn ƒë√¢y ƒë√£ c√≥ s·ª± tƒÉng tr∆∞·ªüng ·∫•n t∆∞·ª£ng v·ªõi t·ªëc ƒë·ªô tƒÉng tr∆∞·ªüng GDP trung b√¨nh kho·∫£ng 6-7% m·ªói nƒÉm. \n",
    "C√°c ng√†nh c√¥ng nghi·ªáp ch·ªß l·ª±c bao g·ªìm s·∫£n xu·∫•t, c√¥ng ngh·ªá th√¥ng tin, n√¥ng nghi·ªáp v√† du l·ªãch. \n",
    "Vi·ªát Nam ƒë√£ k√Ω k·∫øt nhi·ªÅu hi·ªáp ƒë·ªãnh th∆∞∆°ng m·∫°i t·ª± do quan tr·ªçng nh∆∞ CPTPP, EVFTA, gi√∫p m·ªü r·ªông th·ªã tr∆∞·ªùng xu·∫•t kh·∫©u.\"\"\"\n",
    "prompt = f\"VƒÉn b·∫£n: {text2}\\nT√≥m t·∫Øt:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"K·∫øt qu·∫£:\", result.split(\"T√≥m t·∫Øt:\")[-1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2661acdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt qu·∫£: Vi·ªát Nam c√≥ 8 di s·∫£n thi√™n nhi√™n th·∫ø gi·ªõi v√† 3 di s·∫£n vƒÉn h√≥a th·∫ø gi·ªõi. ƒêa d·∫°ng sinh h·ªçc, kh√≠ h·∫≠u v√† ƒë·ªãa h√¨nh ƒë·∫∑c tr∆∞ng c·ªßa Vi·ªát Nam ƒë√£ t·∫°o n√™n nhi·ªÅu ƒëi·ªÉm ƒë·∫øn h·∫•p d·∫´n. Du kh√°ch qu·ªëc t·∫ø ƒë√£ ƒë·∫øn nhi·ªÅu n∆°i v·ªõi nhi·ªÅu chuy·∫øn du l·ªãch. ƒêa d·∫°ng sinh h·ªçc, kh√≠ h·∫≠u v√† ƒë·ªãa h√¨nh ƒë·∫∑c tr∆∞ng c·ªßa Vi·ªát Nam ƒë√£ t·∫°o n√™n nhi·ªÅu ƒëi·ªÉm ƒë·∫øn h·∫•p d·∫´n. Du kh√°ch qu·ªëc t·∫ø ƒë√£ ƒë·∫øn nhi·ªÅu n∆°i v·ªõi nhi·ªÅu chuy·∫øn du l·ªãch.\n"
     ]
    }
   ],
   "source": [
    "text3 = \"\"\"Vi·ªát Nam s·ªü h·ªØu nhi·ªÅu danh lam th·∫Øng c·∫£nh n·ªïi ti·∫øng thu h√∫t du kh√°ch qu·ªëc t·∫ø. \n",
    "·ªû mi·ªÅn B·∫Øc c√≥ V·ªãnh H·∫° Long - di s·∫£n thi√™n nhi√™n th·∫ø gi·ªõi, Sa Pa v·ªõi nh·ªØng th·ª≠a ru·ªông b·∫≠c thang. \n",
    "Mi·ªÅn Trung c√≥ c·ªë ƒë√¥ Hu·∫ø, ph·ªë c·ªï H·ªôi An, c√≤n mi·ªÅn Nam c√≥ ƒë·ªìng b·∫±ng s√¥ng C·ª≠u Long v·ªõi c√°c khu ch·ª£ n·ªïi ƒë·∫∑c s·∫Øc v√† h·ªá th·ªëng s√¥ng ng√≤i ch·∫±ng ch·ªãt.\"\"\"\n",
    "\n",
    "prompt = f\"VƒÉn b·∫£n: {text3}\\nT√≥m t·∫Øt:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"K·∫øt qu·∫£:\", result.split(\"T√≥m t·∫Øt:\")[-1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
