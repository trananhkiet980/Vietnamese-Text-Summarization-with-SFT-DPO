{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10419a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u 300 samples v√†o file: dpo_dataset.csv\n",
      "üìä Th√¥ng tin dataset:\n",
      "- S·ªë l∆∞·ª£ng samples: 300\n",
      "- C√°c c·ªôt: ['prompt', 'chosen', 'rejected']\n",
      "\n",
      "üîç 3 samples ƒë·∫ßu ti√™n:\n",
      "                                              prompt  \\\n",
      "0  T√≥m t·∫Øt vƒÉn b·∫£n sau: ƒê√¢y l√† m·ªôt trong nh·ªØng n·ªô...   \n",
      "1  T√≥m t·∫Øt vƒÉn b·∫£n sau: Chi·ªÅu 12/3 , √¥ng V≈© H√πng ...   \n",
      "2  T√≥m t·∫Øt vƒÉn b·∫£n sau: Tho·∫°t ƒë·∫ßu nh√¨n v√†o b·ª©c ·∫£n...   \n",
      "\n",
      "                                              chosen  \\\n",
      "0  C√°c qu·∫≠n , huy·ªán , th·ªã x√£ tuy√™n truy·ªÅn b·∫±ng nh...   \n",
      "1  Ch√°u b√© ƒë∆∞·ª£c ph√°t hi·ªán trong t√¨nh tr·∫°ng ch∆∞a c...   \n",
      "2  B√°o tuy·∫øt ( Panthera uncia ) ƒë∆∞·ª£c m·ªánh danh l√†...   \n",
      "\n",
      "                                            rejected  \n",
      "0                 T√¥i kh√¥ng th·ªÉ t√≥m t·∫Øt vƒÉn b·∫£n n√†y.  \n",
      "1  Chi·ªÅu 12/3 , √¥ng V≈© H√πng Tri·ªÅu , Tr∆∞·ªüng ph√≤ng ...  \n",
      "2  Tho·∫°t ƒë·∫ßu nh√¨n v√†o b·ª©c ·∫£nh , n·∫øu kh√¥ng c√≥ d·∫•u ...  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "\n",
    "# C·∫•u h√¨nh Google AI Studio\n",
    "API_KEY = \"AIzaSyAig_vvokUlIsA895BDVHvDHbBg2IPmiHk\"\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "def create_dpo_dataset_from_existing():\n",
    "    ds = load_dataset(\"OpenHust/vietnamese-summarization\", split=\"train[:300]\")\n",
    "    dpo_samples = []\n",
    "    \n",
    "    for i, sample in enumerate(ds):\n",
    "        # Ki·ªÉm tra data kh√¥ng r·ªóng\n",
    "        if not sample.get('Document') or not sample.get('Summary'):\n",
    "            continue\n",
    "            \n",
    "        prompt = f\"T√≥m t·∫Øt vƒÉn b·∫£n sau: {sample['Document']}\"\n",
    "        chosen = sample['Summary']\n",
    "        rejected = create_bad_summary_with_gemini(sample['Document'], sample['Summary'])\n",
    "        \n",
    "        # ƒê·∫£m b·∫£o kh√¥ng tr√πng l·∫∑p v√† c√≥ k·∫øt qu·∫£\n",
    "        if chosen != rejected and rejected and len(rejected.strip()) > 10:\n",
    "            dpo_samples.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"chosen\": chosen, \n",
    "                \"rejected\": rejected\n",
    "            }) \n",
    "    return dpo_samples\n",
    "\n",
    "def create_bad_summary_with_gemini(text, good_summary):\n",
    "    \"\"\"S·ª≠ d·ª•ng Gemini ƒë·ªÉ t·∫°o summary k√©m ch·∫•t l∆∞·ª£ng m·ªôt c√°ch th√¥ng minh\"\"\"\n",
    "    \n",
    "    # C√°c chi·∫øn l∆∞·ª£c t·∫°o summary k√©m\n",
    "    strategies = [\n",
    "        f\"\"\"H√£y t·∫°o m·ªôt b·∫£n t√≥m t·∫Øt K√âM CH·∫§T L∆Ø·ª¢NG cho vƒÉn b·∫£n sau. Y√™u c·∫ßu:\n",
    "        - Ch·ªâ ch·ª©a th√¥ng tin ph·ª•, b·ªè s√≥t th√¥ng tin quan tr·ªçng\n",
    "        - Di·ªÖn ƒë·∫°t l·ªßng c·ªßng, kh√≥ hi·ªÉu\n",
    "        - Kh√¥ng n√™u ƒë∆∞·ª£c √Ω ch√≠nh\n",
    "        - ƒê·ªô d√†i kho·∫£ng 30-50 t·ª´\n",
    "        \n",
    "        VƒÉn b·∫£n: {text[:1000]}  # Gi·ªõi h·∫°n ƒë·ªô d√†i ƒë·∫ßu v√†o\"\"\",\n",
    "        \n",
    "        f\"\"\"T·∫°o m·ªôt b·∫£n t√≥m t·∫Øt T·ªíI v·ªõi c√°c ƒë·∫∑c ƒëi·ªÉm:\n",
    "        - T·∫≠p trung v√†o chi ti·∫øt kh√¥ng quan tr·ªçng\n",
    "        - B·ªè qua c√°c ƒëi·ªÉm ch√≠nh\n",
    "        - S·ª≠ d·ª•ng t·ª´ ng·ªØ m∆° h·ªì, kh√¥ng r√µ r√†ng\n",
    "        - C·∫•u tr√∫c c√¢u l·ªôn x·ªôn\n",
    "        \n",
    "        VƒÉn b·∫£n g·ªëc: {text[:1000]}\"\"\",\n",
    "        \n",
    "        f\"\"\"Vi·∫øt m·ªôt b·∫£n t√≥m t·∫Øt CH·∫§T L∆Ø·ª¢NG TH·∫§P b·∫±ng ti·∫øng Vi·ªát v·ªõi:\n",
    "        - Th√¥ng tin sai l·ªách nh·∫π\n",
    "        - Tr√¨nh b√†y kh√¥ng logic\n",
    "        - Thi·∫øu th√¥ng tin quan tr·ªçng\n",
    "        - D√πng t·ª´ l·∫∑p l·∫°i nhi·ªÅu l·∫ßn\n",
    "        \n",
    "        N·ªôi dung: {text[:1000]}\"\"\"\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # Ch·ªçn ng·∫´u nhi√™n m·ªôt chi·∫øn l∆∞·ª£c\n",
    "        selected_strategy = random.choice(strategies)\n",
    "        \n",
    "        # G·ªçi Gemini API\n",
    "        model = genai.GenerativeModel('gemini-pro')\n",
    "        response = model.generate_content(selected_strategy)\n",
    "        \n",
    "        if response and response.text:\n",
    "            bad_summary = response.text.strip()\n",
    "            \n",
    "            # ƒê·∫£m b·∫£o summary kh√¥ng gi·ªëng v·ªõi good summary\n",
    "            if bad_summary != good_summary and len(bad_summary) > 20:\n",
    "                return bad_summary\n",
    "        \n",
    "        # Fallback: t·∫°o summary ƒë∆°n gi·∫£n n·∫øu API fail\n",
    "        return create_fallback_bad_summary(text)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"L·ªói khi g·ªçi Gemini API: {e}\")\n",
    "        # Fallback strategy\n",
    "        return create_fallback_bad_summary(text)\n",
    "\n",
    "def create_fallback_bad_summary(text):\n",
    "    \"\"\"Fallback strategy khi Gemini kh√¥ng ho·∫°t ƒë·ªông\"\"\"\n",
    "    sentences = text.split('. ')\n",
    "    strategies = [\n",
    "        text[:50] + \"...\",                                  # C·∫Øt ng·∫Øn\n",
    "        \"B√†i vi·∫øt n√≥i v·ªÅ nhi·ªÅu ch·ªß ƒë·ªÅ.\",                    # Qu√° chung chung  \n",
    "        \"N·ªôi dung kh√° d√†i v√† ph·ª©c t·∫°p.\",                    # Kh√¥ng c·ª• th·ªÉ\n",
    "        f\"VƒÉn b·∫£n c√≥ {len(text)} k√Ω t·ª±.\",                   # Th√¥ng tin v√¥ d·ª•ng\n",
    "        \". \".join(sentences[:1]) if sentences else text[:100],  # L·∫•y c√¢u ƒë·∫ßu\n",
    "        \"T√¥i kh√¥ng th·ªÉ t√≥m t·∫Øt vƒÉn b·∫£n n√†y.\",               # T·ª´ ch·ªëi\n",
    "        \"N·ªôi dung qu√° ph·ª©c t·∫°p ƒë·ªÉ hi·ªÉu.\"                    # B·ªè cu·ªôc\n",
    "    ]\n",
    "    return random.choice(strategies)\n",
    "\n",
    "# T·∫°o dataset\n",
    "dpo_data = create_dpo_dataset_from_existing()\n",
    "\n",
    "# L∆∞u th√†nh file CSV\n",
    "df = pd.DataFrame(dpo_data)\n",
    "df.to_csv(\"dpo_dataset_gemini.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u {len(dpo_data)} samples v√†o file: dpo_dataset_gemini.csv\")\n",
    "print(\"üìä Th√¥ng tin dataset:\")\n",
    "print(f\"- S·ªë l∆∞·ª£ng samples: {len(df)}\")\n",
    "print(f\"- C√°c c·ªôt: {list(df.columns)}\")\n",
    "print(\"\\nüîç 3 samples ƒë·∫ßu ti√™n:\")\n",
    "for i, row in df.head(3).iterrows():\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(f\"Prompt: {row['prompt'][:100]}...\")\n",
    "    print(f\"Chosen: {row['chosen'][:100]}...\")\n",
    "    print(f\"Rejected: {row['rejected'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34cdc893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers: 4.57.1\n",
      "PEFT: 0.9.0\n",
      "TRL: 0.25.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import peft\n",
    "import trl\n",
    "print(f\"Transformers: {transformers.__version__}\")\n",
    "print(f\"PEFT: {peft.__version__}\") \n",
    "print(f\"TRL: {trl.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12dd968d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caa004cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Loaded dataset rows: 300\n",
      "[+] Sample row: {'prompt': 'T√≥m t·∫Øt vƒÉn b·∫£n sau: ƒê√¢y l√† m·ªôt trong nh·ªØng n·ªôi dung t·∫°i vƒÉn b·∫£n v·ª´a ƒë∆∞·ª£c UBND TP H√† N·ªôi ban h√†nh v·ªÅ vi·ªác tƒÉng c∆∞·ªùng c√¥ng t√°c qu·∫£n l√Ω nu√¥i , gi·∫øt m·ªï , kinh doanh v√† s·ª≠ d·ª•ng th·ªãt ch√≥ , m√®o tr√™n ƒë·ªãa b√†n .Theo ƒë√≥ , c√°c s·ªü , ng√†nh tr√™n ƒë·ªãa b√†n ph·∫£i v√†o cu·ªôc ngay ƒë·ªÉ h∆∞·ªõng t·ªõi thay ƒë·ªïi th√≥i quen c·ªßa ng∆∞·ªùi d√¢n khi d√πng ch√≥ , m√®o l√†m th·ª±c ph·∫©m .G√¢y ph·∫£n c·∫£m v·ªõi du kh√°ch , ng∆∞·ªùi n∆∞·ªõc ngo√†iC≈©ng trong vƒÉn b·∫£n n√†y , UBND TP H√† N·ªôi th·ª´a nh·∫≠n r·∫±ng vi·ªác kinh doanh , gi·∫øt m·ªï v√† s·ª≠ d·ª•ng th·ªãt ch√≥ , m√®o t·∫°i H√† N·ªôi th·ªùi gian qua ƒë√£ t·∫°o ra nh·ªØng h√¨nh ·∫£nh ph·∫£n c·∫£m ƒë·ªëi v·ªõi kh√°ch tham quan du l·ªãch , ng∆∞·ªùi n∆∞·ªõc ngo√†i ƒëang sinh s·ªëng v√† l√†m vi·ªác t·∫°i H√† N·ªôi , g√¢y ·∫£nh h∆∞·ªüng t·ªõi h√¨nh ·∫£nh c·ªßa m·ªôt th·ªß ƒë√¥ \" vƒÉn minh , hi·ªán ƒë·∫°i \" .Trong th·ª±c t·∫ø , theo ghi nh·∫≠n c·ªßa Tu·ªïi Tr·∫ª t·∫°i ph·ªë Tam Trinh , ngay ƒëo·∫°n ƒë·∫ßu c·∫ßu Mai ƒê·ªông ( qu·∫≠n Ho√†ng Mai ) , m·ªôt ƒëo·∫°n ph·ªë d√†i v·ªõi g·∫ßn ch·ª•c c·ª≠a h√†ng bu√¥n b√°n th·ªãt ch√≥ s·ªëng , nh√† h√†ng ph·ª•c v·ª• c√°c m√≥n ch·∫ø bi·∫øn t·ª´ th·ªãt ch√≥ v·∫´n ho·∫°t ƒë·ªông t·∫•p n·∫≠p nhi·ªÅu nƒÉm nay .Ch·ªã Th . , m·ªôt ch·ªß s·∫°p b√°n th·ªãt ch√≥ s·ªëng ·ªü ƒë√¢y , cho hay trong 10 ng√†y ƒë·∫ßu m·ªói th√°ng √¢m l·ªãch , c√°c c·ª≠a h√†ng v·∫Øng v·∫ª h∆°n v√¨ th√≥i quen \" ki√™ng \" ƒÉn th·ªãt ch√≥ v√†o ƒë·∫ßu th√°ng tr√°nh xui x·∫ªo .Ri√™ng nh·ªØng ng√†y cu·ªëi th√°ng , c√°c c·ª≠a h√†ng ·ªü ƒë√¢y t·∫•p n·∫≠p ng∆∞·ªùi mua b√°n .Theo ch·ªã Th . , trung b√¨nh m·ªói th√°ng ch·ªã b√°n ƒë∆∞·ª£c kho·∫£ng 3 t·∫° th·ªãt ch√≥ s·ªëng .T∆∞∆°ng t·ª± , t·∫°i ph·ªë Nguy·ªÖn Khang ( qu·∫≠n C·∫ßu Gi·∫•y ) , h√†ng lo·∫°t nh√† h√†ng th·ªãt ch√≥ quy m√¥ l·ªõn ( 1-5 t·∫ßng ) nh∆∞ S∆°n H·∫£i , Chi·∫øu Hoa , M∆° Hoa qu√°n ... lu√¥n t·∫•p n·∫≠p , m·ªói ng√†y ti·∫øp c·∫£ ngh√¨n th·ª±c kh√°ch .Tuy nhi√™n , h√†ng ch·ª•c qu√°n th·ªãt ch√≥ tr√™n ƒë∆∞·ªùng √Çu C∆° ( ph∆∞·ªùng Nh·∫≠t T√¢n , qu·∫≠n T√¢y H·ªì ) t·ª´ng n·ªïi ti·∫øng m·ªôt th·ªùi nh∆∞ Tr·∫ßn M·ª•c , H·ªì Ki·∫øm , A Trang , Anh T√∫ X·ªãn , Anh T√∫ Nh√† K√≠nh , Anh T√∫ Nh√† L√° ... ƒë√£ ƒë√≥ng c·ª≠a kh√¥ng r√µ l√Ω do , ch·ªâ c√≤n duy nh·∫•t c·ª≠a h√†ng c√≥ t√™n Anh T√∫ B√©o .Nhi·ªÅu nguy c∆° l√¢y lan d·ªãch b·ªánhTheo chuy√™n gia truy·ªÅn th√¥ng Phan Ki·ªÅn , ch√≥ kh√¥ng ph·∫£i l√† lo√†i ƒë·ªông v·∫≠t n·∫±m trong danh m·ª•c c·∫•m sƒÉn b·∫Øt , mua b√°n v√† gi·∫øt m·ªï , th·ªãt ch√≥ th∆∞∆°ng ph·∫©m c≈©ng kh√¥ng ph·∫£i l√† m·∫∑t h√†ng th·ª±c ph·∫©m c·∫•m l∆∞u h√†nh n√™n r·∫•t kh√≥ d√πng lu·∫≠t ƒë·ªÉ c·∫•m .Do ƒë√≥ , vi·ªác H√† N·ªôi khuy·∫øn kh√≠ch ng∆∞·ªùi d√¢n h·∫°n ch·∫ø , ti·∫øn t·ªõi kh√¥ng ƒÉn th·ªãt ch√≥ l√† c√°ch l√†m v·ª´a vƒÉn minh v·ª´aƒë√∫ng lu·∫≠t .\" Ho√†n to√†n c√≥ th·ªÉ thay ƒë·ªïi m·ªôt th√≥i quen x·∫•u n·∫øu ƒë∆∞·ª£c truy·ªÅn th√¥ng t·ªët , m√† vi·ªác b·ªè ƒÉn ti·∫øt canh l√† m·ªôt v√≠ d·ª• .Tr∆∞·ªõc ƒë√¢y c√°c h√†ng qu√°n \" l√≤ng l·ª£n , ti·∫øt canh \" m·ªçc kh·∫Øp n∆°i b·ªüi ƒë√¢y l√† m√≥n kho√°i kh·∫©u c·ªßa d√¢n nh·∫≠u .Nh∆∞ng khi b√°o ch√≠ ƒë∆∞a tin nhi·ªÅu v·ªÅ c√°c ca nhi·ªÖm khu·∫©n c·∫ßu l·ª£n g√¢y t·ª≠ vong v·ªõi nh·ªØng tri·ªáu ch·ª©ng kh·ªßng khi·∫øp , g·∫ßn nh∆∞ m√≥n ƒÉn n√†y b·ªã t·∫©y chay , hi·ªán r·∫•t √≠t ng∆∞·ªùi d√πng m√≥n ƒÉn n√†y \" - v·ªã n√†y n√≥i .Trong khi ƒë√≥ , kh√¥ng ch·ªâ nh·ªØng b·∫°n tr·∫ª nu√¥i ch√≥ c·∫£nh l√†m th√∫ c∆∞ng ph·∫£n ƒë·ªëi vi·ªác xem ch√≥ l√† m·ªôt lo·∫°i th·ª±c ph·∫©m , m√† ngay c·∫£ nh·ªØng ng∆∞·ªùi t·ª´ng xem th·ªãt ch√≥ l√† m√≥n \" kho√°i kh·∫©u \" c≈©ng ·ªßng h·ªô vi·ªác b·ªè ƒÉn th·ªãt ch√≥ .D√π t·ª´ng ƒÉn th·ªãt ch√≥ , anh Nguy·ªÖn VƒÉn Thu·∫≠n , m·ªôt nh√¢n vi√™n vƒÉn ph√≤ng ·ªü qu·∫≠n Thanh Xu√¢n , c≈©ng ·ªßng h·ªô ch·ªß tr∆∞∆°ng c·ªßa H√† N·ªôi .Theo anh Thu·∫≠n , m·ªôt khi th·ªãt ch√≥ c√≤n th·ªãnh h√†nh v√† c√≥ gi√° tr·ªã th∆∞∆°ng ph·∫©m cao , n·∫°n tr·ªôm ch√≥ v·∫´n t·ªìn t·∫°i v√† kh√≥ tr√°nh kh·ªèi c·∫£nh ng∆∞·ªùi tr·ªôm ch√≥ b·ªã ƒë√°nh \" th·ª´a s·ªëng thi·∫øu ch·∫øt \" nh∆∞ ƒë√£ t·ª´ng x·∫£y ra .Ngo√†i ra nguy c∆° l√¢y lan d·ªãch b·ªánh , ƒë·∫∑c bi·ªát l√† b·ªánh d·∫°i , do ho·∫°t ƒë·ªông bu√¥n b√°n v√† v·∫≠n chuy·ªÉn ch√≥ s·ªëng gi·ªØa c√°c ƒë·ªãa ph∆∞∆°ng c≈©ng l√† v·∫•n ƒë·ªÅ ƒë√°ng lo ng·∫°i .Do ƒë√≥ , mu·ªën h·∫°n ch·∫ø n·∫°n tr·ªôm ch√≥ v√† ngƒÉn ng·ª´a nguy c∆° l√¢y lan d·ªãch b·ªánh , c·∫ßn khuy·∫øn kh√≠ch kh√¥ng n√™n ƒÉn th·ªãt ch√≥ .\" Th·ªãt ch√≥ c≈©ng ngon v√† l·∫° mi·ªáng nh∆∞ng n·∫øu c·∫•m ƒÉn th·ªãt ch√≥ , m√¨nh th·∫•y c≈©ng kh√¥ng v·∫•n ƒë·ªÅ g√¨ v√¨ c√≤n r·∫•t nhi·ªÅu lo·∫°i th·ª±c ph·∫©m kh√°c thay th·∫ø \" - anh Thu·∫≠n n√≥i .', 'chosen': 'C√°c qu·∫≠n , huy·ªán , th·ªã x√£ tuy√™n truy·ªÅn b·∫±ng nhi·ªÅu h√¨nh th·ª©c ƒë·ªÉ ng∆∞·ªùi d√¢n t·ª´ b·ªè th√≥i quen ƒÉn th·ªãt ch√≥ , m√®o nh·∫±m tr√°nh nguy c∆° m·∫Øc b·ªánh truy·ªÅn nhi·ªÖm ( b·ªánh d·∫°i , xo·∫Øn khu·∫©n , t·∫£ ... ) c≈©ng nh∆∞ kh√¥ng g√¢y ·∫£nh h∆∞·ªüng t·ªõi h√¨nh ·∫£nh th·ªß ƒë√¥ vƒÉn minh , hi·ªán ƒë·∫°i .', 'rejected': 'T√¥i kh√¥ng th·ªÉ t√≥m t·∫Øt vƒÉn b·∫£n n√†y.'}\n",
      "[+] Loading base model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Applying PEFT adapter from: runs/qwen3-0.6B-summarization-lora\n",
      "trainable params: 4,587,520 || all params: 600,637,440 || trainable%: 0.7637752318603382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting prompt in train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 9792.76 examples/s]\n",
      "Applying chat template to train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 15374.45 examples/s]\n",
      "Tokenizing train dataset: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 635.68 examples/s]\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'bos_token_id': None, 'pad_token_id': 151643}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Trainer created. Doing small checks...\n",
      "[+] Any parameter requires_grad: True\n",
      "[+] Sample raw example (should have prompt, chosen, rejected): {'prompt': 'T√≥m t·∫Øt vƒÉn b·∫£n sau: ƒê√¢y l√† m·ªôt trong nh·ªØng n·ªôi dung t·∫°i vƒÉn b·∫£n v·ª´a ƒë∆∞·ª£c UBND TP H√† N·ªôi ban h√†nh v·ªÅ vi·ªác tƒÉng c∆∞·ªùng c√¥ng t√°c qu·∫£n l√Ω nu√¥i , gi·∫øt m·ªï , kinh doanh v√† s·ª≠ d·ª•ng th·ªãt ch√≥ , m√®o tr√™n ƒë·ªãa b√†n .Theo ƒë√≥ , c√°c s·ªü , ng√†nh tr√™n ƒë·ªãa b√†n ph·∫£i v√†o cu·ªôc ngay ƒë·ªÉ h∆∞·ªõng t·ªõi thay ƒë·ªïi th√≥i quen c·ªßa ng∆∞·ªùi d√¢n khi d√πng ch√≥ , m√®o l√†m th·ª±c ph·∫©m .G√¢y ph·∫£n c·∫£m v·ªõi du kh√°ch , ng∆∞·ªùi n∆∞·ªõc ngo√†iC≈©ng trong vƒÉn b·∫£n n√†y , UBND TP H√† N·ªôi th·ª´a nh·∫≠n r·∫±ng vi·ªác kinh doanh , gi·∫øt m·ªï v√† s·ª≠ d·ª•ng th·ªãt ch√≥ , m√®o t·∫°i H√† N·ªôi th·ªùi gian qua ƒë√£ t·∫°o ra nh·ªØng h√¨nh ·∫£nh ph·∫£n c·∫£m ƒë·ªëi v·ªõi kh√°ch tham quan du l·ªãch , ng∆∞·ªùi n∆∞·ªõc ngo√†i ƒëang sinh s·ªëng v√† l√†m vi·ªác t·∫°i H√† N·ªôi , g√¢y ·∫£nh h∆∞·ªüng t·ªõi h√¨nh ·∫£nh c·ªßa m·ªôt th·ªß ƒë√¥ \" vƒÉn minh , hi·ªán ƒë·∫°i \" .Trong th·ª±c t·∫ø , theo ghi nh·∫≠n c·ªßa Tu·ªïi Tr·∫ª t·∫°i ph·ªë Tam Trinh , ngay ƒëo·∫°n ƒë·∫ßu c·∫ßu Mai ƒê·ªông ( qu·∫≠n Ho√†ng Mai ) , m·ªôt ƒëo·∫°n ph·ªë d√†i v·ªõi g·∫ßn ch·ª•c c·ª≠a h√†ng bu√¥n b√°n th·ªãt ch√≥ s·ªëng , nh√† h√†ng ph·ª•c v·ª• c√°c m√≥n ch·∫ø bi·∫øn t·ª´ th·ªãt ch√≥ v·∫´n ho·∫°t ƒë·ªông t·∫•p n·∫≠p nhi·ªÅu nƒÉm nay .Ch·ªã Th . , m·ªôt ch·ªß s·∫°p b√°n th·ªãt ch√≥ s·ªëng ·ªü ƒë√¢y , cho hay trong 10 ng√†y ƒë·∫ßu m·ªói th√°ng √¢m l·ªãch , c√°c c·ª≠a h√†ng v·∫Øng v·∫ª h∆°n v√¨ th√≥i quen \" ki√™ng \" ƒÉn th·ªãt ch√≥ v√†o ƒë·∫ßu th√°ng tr√°nh xui x·∫ªo .Ri√™ng nh·ªØng ng√†y cu·ªëi th√°ng , c√°c c·ª≠a h√†ng ·ªü ƒë√¢y t·∫•p n·∫≠p ng∆∞·ªùi mua b√°n .Theo ch·ªã Th . , trung b√¨nh m·ªói th√°ng ch·ªã b√°n ƒë∆∞·ª£c kho·∫£ng 3 t·∫° th·ªãt ch√≥ s·ªëng .T∆∞∆°ng t·ª± , t·∫°i ph·ªë Nguy·ªÖn Khang ( qu·∫≠n C·∫ßu Gi·∫•y ) , h√†ng lo·∫°t nh√† h√†ng th·ªãt ch√≥ quy m√¥ l·ªõn ( 1-5 t·∫ßng ) nh∆∞ S∆°n H·∫£i , Chi·∫øu Hoa , M∆° Hoa qu√°n ... lu√¥n t·∫•p n·∫≠p , m·ªói ng√†y ti·∫øp c·∫£ ngh√¨n th·ª±c kh√°ch .Tuy nhi√™n , h√†ng ch·ª•c qu√°n th·ªãt ch√≥ tr√™n ƒë∆∞·ªùng √Çu C∆° ( ph∆∞·ªùng Nh·∫≠t T√¢n , qu·∫≠n T√¢y H·ªì ) t·ª´ng n·ªïi ti·∫øng m·ªôt th·ªùi nh∆∞ Tr·∫ßn M·ª•c , H·ªì Ki·∫øm , A Trang , Anh T√∫ X·ªãn , Anh T√∫ Nh√† K√≠nh , Anh T√∫ Nh√† L√° ... ƒë√£ ƒë√≥ng c·ª≠a kh√¥ng r√µ l√Ω do , ch·ªâ c√≤n duy nh·∫•t c·ª≠a h√†ng c√≥ t√™n Anh T√∫ B√©o .Nhi·ªÅu nguy c∆° l√¢y lan d·ªãch b·ªánhTheo chuy√™n gia truy·ªÅn th√¥ng Phan Ki·ªÅn , ch√≥ kh√¥ng ph·∫£i l√† lo√†i ƒë·ªông v·∫≠t n·∫±m trong danh m·ª•c c·∫•m sƒÉn b·∫Øt , mua b√°n v√† gi·∫øt m·ªï , th·ªãt ch√≥ th∆∞∆°ng ph·∫©m c≈©ng kh√¥ng ph·∫£i l√† m·∫∑t h√†ng th·ª±c ph·∫©m c·∫•m l∆∞u h√†nh n√™n r·∫•t kh√≥ d√πng lu·∫≠t ƒë·ªÉ c·∫•m .Do ƒë√≥ , vi·ªác H√† N·ªôi khuy·∫øn kh√≠ch ng∆∞·ªùi d√¢n h·∫°n ch·∫ø , ti·∫øn t·ªõi kh√¥ng ƒÉn th·ªãt ch√≥ l√† c√°ch l√†m v·ª´a vƒÉn minh v·ª´aƒë√∫ng lu·∫≠t .\" Ho√†n to√†n c√≥ th·ªÉ thay ƒë·ªïi m·ªôt th√≥i quen x·∫•u n·∫øu ƒë∆∞·ª£c truy·ªÅn th√¥ng t·ªët , m√† vi·ªác b·ªè ƒÉn ti·∫øt canh l√† m·ªôt v√≠ d·ª• .Tr∆∞·ªõc ƒë√¢y c√°c h√†ng qu√°n \" l√≤ng l·ª£n , ti·∫øt canh \" m·ªçc kh·∫Øp n∆°i b·ªüi ƒë√¢y l√† m√≥n kho√°i kh·∫©u c·ªßa d√¢n nh·∫≠u .Nh∆∞ng khi b√°o ch√≠ ƒë∆∞a tin nhi·ªÅu v·ªÅ c√°c ca nhi·ªÖm khu·∫©n c·∫ßu l·ª£n g√¢y t·ª≠ vong v·ªõi nh·ªØng tri·ªáu ch·ª©ng kh·ªßng khi·∫øp , g·∫ßn nh∆∞ m√≥n ƒÉn n√†y b·ªã t·∫©y chay , hi·ªán r·∫•t √≠t ng∆∞·ªùi d√πng m√≥n ƒÉn n√†y \" - v·ªã n√†y n√≥i .Trong khi ƒë√≥ , kh√¥ng ch·ªâ nh·ªØng b·∫°n tr·∫ª nu√¥i ch√≥ c·∫£nh l√†m th√∫ c∆∞ng ph·∫£n ƒë·ªëi vi·ªác xem ch√≥ l√† m·ªôt lo·∫°i th·ª±c ph·∫©m , m√† ngay c·∫£ nh·ªØng ng∆∞·ªùi t·ª´ng xem th·ªãt ch√≥ l√† m√≥n \" kho√°i kh·∫©u \" c≈©ng ·ªßng h·ªô vi·ªác b·ªè ƒÉn th·ªãt ch√≥ .D√π t·ª´ng ƒÉn th·ªãt ch√≥ , anh Nguy·ªÖn VƒÉn Thu·∫≠n , m·ªôt nh√¢n vi√™n vƒÉn ph√≤ng ·ªü qu·∫≠n Thanh Xu√¢n , c≈©ng ·ªßng h·ªô ch·ªß tr∆∞∆°ng c·ªßa H√† N·ªôi .Theo anh Thu·∫≠n , m·ªôt khi th·ªãt ch√≥ c√≤n th·ªãnh h√†nh v√† c√≥ gi√° tr·ªã th∆∞∆°ng ph·∫©m cao , n·∫°n tr·ªôm ch√≥ v·∫´n t·ªìn t·∫°i v√† kh√≥ tr√°nh kh·ªèi c·∫£nh ng∆∞·ªùi tr·ªôm ch√≥ b·ªã ƒë√°nh \" th·ª´a s·ªëng thi·∫øu ch·∫øt \" nh∆∞ ƒë√£ t·ª´ng x·∫£y ra .Ngo√†i ra nguy c∆° l√¢y lan d·ªãch b·ªánh , ƒë·∫∑c bi·ªát l√† b·ªánh d·∫°i , do ho·∫°t ƒë·ªông bu√¥n b√°n v√† v·∫≠n chuy·ªÉn ch√≥ s·ªëng gi·ªØa c√°c ƒë·ªãa ph∆∞∆°ng c≈©ng l√† v·∫•n ƒë·ªÅ ƒë√°ng lo ng·∫°i .Do ƒë√≥ , mu·ªën h·∫°n ch·∫ø n·∫°n tr·ªôm ch√≥ v√† ngƒÉn ng·ª´a nguy c∆° l√¢y lan d·ªãch b·ªánh , c·∫ßn khuy·∫øn kh√≠ch kh√¥ng n√™n ƒÉn th·ªãt ch√≥ .\" Th·ªãt ch√≥ c≈©ng ngon v√† l·∫° mi·ªáng nh∆∞ng n·∫øu c·∫•m ƒÉn th·ªãt ch√≥ , m√¨nh th·∫•y c≈©ng kh√¥ng v·∫•n ƒë·ªÅ g√¨ v√¨ c√≤n r·∫•t nhi·ªÅu lo·∫°i th·ª±c ph·∫©m kh√°c thay th·∫ø \" - anh Thu·∫≠n n√≥i .', 'chosen': 'C√°c qu·∫≠n , huy·ªán , th·ªã x√£ tuy√™n truy·ªÅn b·∫±ng nhi·ªÅu h√¨nh th·ª©c ƒë·ªÉ ng∆∞·ªùi d√¢n t·ª´ b·ªè th√≥i quen ƒÉn th·ªãt ch√≥ , m√®o nh·∫±m tr√°nh nguy c∆° m·∫Øc b·ªánh truy·ªÅn nhi·ªÖm ( b·ªánh d·∫°i , xo·∫Øn khu·∫©n , t·∫£ ... ) c≈©ng nh∆∞ kh√¥ng g√¢y ·∫£nh h∆∞·ªüng t·ªõi h√¨nh ·∫£nh th·ªß ƒë√¥ vƒÉn minh , hi·ªán ƒë·∫°i .', 'rejected': 'T√¥i kh√¥ng th·ªÉ t√≥m t·∫Øt vƒÉn b·∫£n n√†y.'}\n",
      "[+] Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 45:05, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.841100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.465000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.652400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.398400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.593100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.380500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.370500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.356300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.248000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.304800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.263200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.539900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.324000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.228000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.194500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[+] Training finished. Saving model...\n",
      "[+] Saved to runs/dpo-qwen3-0.6B\n"
     ]
    }
   ],
   "source": [
    "# DPO\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from trl import DPOTrainer, DPOConfig\n",
    "from peft import PeftModel\n",
    "\n",
    "BASE_MODEL = \"Qwen/Qwen3-0.6B\"\n",
    "SFT_PEFT_PATH = \"runs/qwen3-0.6B-summarization-lora\"\n",
    "CSV_PATH = \"dpo_dataset.csv\"\n",
    "OUTPUT_DIR = \"runs/dpo-qwen3-0.6B\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_PROMPT_LEN = 128\n",
    "MAX_COMPLETION_LEN = 128\n",
    "BATCH_SIZE = 4\n",
    "EPOCHS = 2\n",
    "FP16 = False  # N·∫øu GPU c≈© ho·∫∑c l·ªói, ƒë·∫∑t False\n",
    "\n",
    "def load_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)\n",
    "    if tokenizer.pad_token is None:\n",
    "        # ƒë·∫∑t pad = eos ƒë·ªÉ tr√°nh l·ªói\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    return tokenizer\n",
    "\n",
    "def load_dataset(csv_path):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required = {\"prompt\", \"chosen\", \"rejected\"}\n",
    "    if not required.issubset(set(df.columns)):\n",
    "        raise ValueError(f\"CSV ph·∫£i c√≥ c·ªôt: {required}. Hi·ªán c√≥: {list(df.columns)}\")\n",
    "\n",
    "    # ƒë·∫£m b·∫£o t·∫•t c·∫£ l√† string, kh√¥ng c√≥ NaN\n",
    "    df = df[['prompt','chosen','rejected']].fillna(\"\").astype(str)\n",
    "\n",
    "    ds = Dataset.from_pandas(df.reset_index(drop=True))\n",
    "    print(f\"[+] Loaded dataset rows: {len(ds)}\")\n",
    "    print(\"[+] Sample row:\", ds[0])\n",
    "    return ds\n",
    "\n",
    "def make_model_and_peft():\n",
    "    print(\"[+] Loading base model...\")\n",
    "    # N·∫øu thi·∫øu VRAM b·∫°n c√≥ th·ªÉ d√πng low_cpu_mem_usage=True ho·∫∑c load_in_8bit (c·∫ßn bitsandbytes)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL,\n",
    "        torch_dtype=torch.float16 if FP16 and device==\"cuda\" else torch.float32,\n",
    "        device_map=\"auto\" if device==\"cuda\" else None,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    print(\"[+] Applying PEFT adapter from:\", SFT_PEFT_PATH)\n",
    "    # N·∫øu adapter ch·ª©a config kh√¥ng t∆∞∆°ng th√≠ch, b·∫°n ƒë√£ fix adapter_config.json tr∆∞·ªõc ƒë√≥\n",
    "    model = PeftModel.from_pretrained(model, SFT_PEFT_PATH, device_map=\"auto\" if device==\"cuda\" else None)\n",
    "\n",
    "    # IMPORTANT: switch to train mode (otherwise grads off)\n",
    "    model.train()\n",
    "\n",
    "    # Freeze base model parameters, unfreeze adapter/LoRA params\n",
    "    # T√™n tham s·ªë LoRA th∆∞·ªùng ch·ª©a 'lora' ho·∫∑c 'adapter' ho·∫∑c 'delta' - ƒëi·ªÅu ch·ªânh n·∫øu adapter c·ªßa b·∫°n kh√°c\n",
    "    trainable = 0\n",
    "    total = 0\n",
    "    for n, p in model.named_parameters():\n",
    "        total += p.numel()\n",
    "        # heuristic: m·ªü train cho nh·ªØng tham s·ªë li√™n quan LoRA/adapter\n",
    "        if (\"lora\" in n.lower()) or (\"adapter\" in n.lower()) or (\"delta\" in n.lower()) or (\"peft\" in n.lower()):\n",
    "            p.requires_grad = True\n",
    "            trainable += p.numel()\n",
    "        else:\n",
    "            p.requires_grad = False\n",
    "\n",
    "    # n·∫øu mu·ªën in chi ti·∫øt:\n",
    "    try:\n",
    "        model.print_trainable_parameters()  # peft helper n·∫øu c√≥\n",
    "    except Exception:\n",
    "        print(f\"[+] Trainable params (approx): {trainable:,} / {total:,}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    tokenizer = load_tokenizer()\n",
    "    dataset = load_dataset(CSV_PATH)\n",
    "\n",
    "    model = make_model_and_peft()\n",
    "\n",
    "    # DPOConfig (tham s·ªë t∆∞∆°ng th√≠ch trl 0.25.0)\n",
    "    training_args = DPOConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        gradient_accumulation_steps=1,\n",
    "        num_train_epochs=EPOCHS,\n",
    "        learning_rate=1e-5,\n",
    "        fp16=FP16 and device==\"cuda\",\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=3,\n",
    "        remove_unused_columns=False,\n",
    "        max_prompt_length=MAX_PROMPT_LEN,\n",
    "        max_completion_length=MAX_COMPLETION_LEN,\n",
    "        gradient_checkpointing=False,\n",
    "        beta=0.1,\n",
    "        loss_type=\"sigmoid\",\n",
    "    )\n",
    "\n",
    "    # DPOTrainer expects raw text columns prompt/chosen/rejected and will do internal tokenization.\n",
    "    trainer = DPOTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    # quick sanity checks before training\n",
    "    print(\"[+] Trainer created. Doing small checks...\")\n",
    "    # check that at least one parameter requires_grad\n",
    "    any_grad = any(p.requires_grad for p in model.parameters())\n",
    "    print(\"[+] Any parameter requires_grad:\", any_grad)\n",
    "    if not any_grad:\n",
    "        raise RuntimeError(\"No trainable parameters detected. Check PEFT adapter and param names.\")\n",
    "\n",
    "    # inspect prepared dataset (DPOTrainer will call its own preprocessing inside train; but we can test tokenization via trainer.tokenize_row)\n",
    "    # Note: tokenization function in DPOTrainer is internal; we do a small sample mapping by calling trainer._prepare_dataset if needed,\n",
    "    # but to keep safe we just print first raw example:\n",
    "    print(\"[+] Sample raw example (should have prompt, chosen, rejected):\", dataset[0])\n",
    "\n",
    "    print(\"[+] Starting training...\")\n",
    "    trainer.train()\n",
    "    print(\"[+] Training finished. Saving model...\")\n",
    "    trainer.save_model(OUTPUT_DIR)\n",
    "    print(\"[+] Saved to\", OUTPUT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5b2506f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K·∫øt qu·∫£: M√≥n ƒÉn ƒë·∫∑c tr∆∞ng c·ªßa mi·ªÅn B·∫Øc l√† ph·ªü. Ph·ªü ƒë∆∞·ª£c n·∫•u t·ª´ g·∫°o t∆∞∆°i, nguy√™n li·ªáu ch√≠nh l√† th·ªãt cua, th·ªãt b√≤ v√† th·ªãt m√®. Th·ªãt cua c√≥ v·ªã cay v√† ng·ªçt; th·ªãt b√≤ c√≥ v·ªã ng·ªçt ƒë·∫≠m v√† cay; th·ªãt m√® c√≥ v·ªã ng·ªçt v√† cay. Trong ph·ªü th∆∞·ªùng c√≥ th√™m n∆∞·ªõc c·ªët t√≠m, n∆∞·ªõc c·ªët m√® v√† n∆∞·ªõc c·ªët m·∫Øm ƒë·ªÉ cay v√† gi·ªØ l·∫°i v·ªã ng·ªçt\n"
     ]
    }
   ],
   "source": [
    "# Code test nhanh\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen3-0.6B\", device_map=\"auto\")\n",
    "model = PeftModel.from_pretrained(model, \"./runs/dpo-qwen3-0.6B\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./runs/dpo-qwen3-0.6B\")\n",
    "\n",
    "text1 = \"\"\"·∫®m th·ª±c Vi·ªát Nam r·∫•t ƒëa d·∫°ng v√† phong ph√∫, v·ªõi nhi·ªÅu m√≥n ƒÉn ƒë·∫∑c tr∆∞ng theo t·ª´ng v√πng mi·ªÅn. \n",
    "Mi·ªÅn B·∫Øc n·ªïi ti·∫øng v·ªõi ph·ªü, b√∫n ch·∫£, mi·ªÅn Trung c√≥ b√∫n b√≤ Hu·∫ø, cao l·∫ßu, c√≤n mi·ªÅn Nam c√≥ h·ªß ti·∫øu, c∆°m t·∫•m. \n",
    "C√°c m√≥n ƒÉn Vi·ªát Nam th∆∞·ªùng ch√∫ tr·ªçng s·ª± c√¢n b·∫±ng gi·ªØa c√°c v·ªã chua, cay, m·∫∑n, ng·ªçt v√† s·ª≠ d·ª•ng nhi·ªÅu rau th∆°m, gia v·ªã t∆∞∆°i.\"\"\"\n",
    "prompt = f\"VƒÉn b·∫£n: {text1}\\nT√≥m t·∫Øt:\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"K·∫øt qu·∫£:\", result.split(\"T√≥m t·∫Øt:\")[-1].strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
